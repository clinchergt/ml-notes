#2016 CS224D
## lecture 1

por que deep learning?

con machine learning normal, se pasa mucho tiempo describiendo la data, ajustando los parametros manualmente, especificamente para ese dataset o en mi caso tal vez incluso para ese especifico lenguaje (e.g ingles). deep learning permite que uno le diga al algoritmo esta es la data, y que el sepa describir y haga un prediccion por su cuenta, sin tener que describir manualmente. encuentra la respuesta si le doy el (raw) input y el output que deberia salir.

deep learning won't solve NLP tho (search andrew's post on reddit)

it also requires *a lot* of data

takes advantage of powerful computers (multicores, gpu help, etc.)

syntactical analysis: as opposed to traditional methods, every word is a vector, so every phrase a vector, so if two phrases are similar, hopefully the two vectors will be similar as well, which gives some meaning, some context to the actual phrase. in traditional methods we would only learn that something is e.g. a noun phrase.

semantical analysis: not having to understand subtle details. in traditional methods we might arrive at very convoluted way to represent (lambda calculus) and it's very discrete. there's no notion of similarity between phrases. in deep learning, you can actually make comparisons and predictions more directly, without having to define lots of subtleties. _everything's a vector_.


skewed distributions still affect deep learning, a good dataset is very important. basic probability, you need a proper representation of reality.

## lecture 2

wordnet has several problems also only english, subjective, subtleties are lost.

discrete representation lose also subtleties, similarities aren't well representative.

distributional similarity based representations: representing the words based on what surrounds them. old idea but very succesful in modern statistical nlp.

but how? collecting large text corpora. analyse n words to left, n words to right, "windows". common windows range from 5 to 10.

when picking how many dimentions, depends on the actual task. testing, see what happens as you go up or down. if the trend is up and i didn't continue, why? no reason not to. explore possibilities.

some very frequent words might skew the results. possible solutions include ignoring the words and capping the number they can get in the co occurrence matrix before applying svd


word2vec:

we try to predict words surrounding a particular word.

you optimize the log probability of any context word given the current center word

not very pretty to have two vectors per word, but it makes optimization easier, at the end you average both vectors out to get the final representation of the word.


## lecture 3

### word2vec; skip-gram model

simple word2vec alg consists of moving the window along the whole corpus (or corpora) and trying to predict the outside words. this means that each window has a center word, and a number of outer words. you try to predict the outer words based on the inner word.

after optimizing you will end up with 2 vectors for each word. one representing it as an outer word and one as a center word. in order to get the final representation you just average them or add them together. not too pretty but works well in practice.

updating the cost however involves calculating the gradient over every word in the corpus at every step. this is very expensive. if deep learning had neat simple convex functions the improvement would be fairly straight forward too.

in order to deal with the expensiveness of gradient descent we do stochastic gradient descent instead. we will almost always update the parameter after each window. small updates at each window.

btw, we initialize the word vectors as vectors of random small numbers (remember cs229)

since the updated matrix is so sparse, we just update the vectors that actually appear.

also, the normalization factor is _very_ expensive to compute as it sums over every word in the vocabulary. we instead we don't get all the words that don't appear in the current window, but just a few random words. it doesn't help much that e.g. 'zebra', doesn't co-occur with 'toaster'.

our cost function the becomes a maximization of the words in the window co-occurring in a log of the sigma function and then minimizing the cost of unrelated words co-occurring using the log of the sigma function of the negative inner product of those word vectors (taking advantage of the sigma(-x) == 1 - sigma(x) equivalence).

we use the sigma function because we're treating this as if it were a logistic regression problem. either words are related or not is basically what we're optimizing.

we also sample more frequent words more frequently in the negative samples as they should be more representative. you take their initial probability of occurring and raise that to the 3/4th power to decide how often to sample them.

#### continuous bag of words

another similar model. you try to predict the center word from the outside words instead of the other way around. it also works well.

you can even combine the skip-gram model and this one and average out the word vectors.

### glove

basically, fill out a huge co-occurrence matrix and use those numbers and subtract them from the word inner products and maximize that in order to get a similar result.

a caveat would be the very common pairs, so we have a maximum in the matrix. we don't want the very large counts to dominate the alg. in order to compensate for the very very common word pairs.

this is very scalable, because we only go over the corpus once. and then do all the math over the matrix, which is a lot smaller than the corpus.

### evaluate vectors

there are two ways, intrinsic and extrinsic evaluations.

the intrinsic tasks are just quick to compute, evaluate a specific task, there has to be some correlation between the metrics however. these help you understand the system.

the extrinsic tasks are actual useful tasks that would take long to compute and evaluate. these are basically real tasks. if you make one small change and the results improve overall, you can say that it was that small element that caused the improvement.

for instance, if you want to gauge how well you've picked the word vector dimensionality, you can vary it and test the result and see if there's any improvement in the measures, but intrinsic and extrinsic. since the former are faster to calculate, this is where they come in handy. again, make sure there's however some correlation between them and the actual real task you'll be doing.

--------------
#2015 CS224D
##svd

- poder de computacion. matrices grandisimas. recursos?

- dificil agregar nuevas palabras/documentos. en un perfilador importante mantener al dia. lenguaje cambia, evoluciona.

- la geometria de las formas de palabras similares. no tienen misma estructura. no permiten hacer e.g. la misma operacion vectorial para llegar al pasado de un verbo.

- no es deep learning por asi decirlo

----

aprender directamente de la data los vectores reducidos.

back propagation ~= chain rule

recent simple faster: word2vec -> no counting, predict surrounding words.

----

##GloVe

intrincsic word vector eval

para responder calcular cosine distance de varios vectores. maximiar la distancia

no incluir el input (e.g. no incluir king en man:woman :: king:?)

depende del corpus, si habla mucho del que es correcto, lo lograra.

ambigÃ¼edad? dejar que el futuro modelo lo maneje *o* usar k means clustering (buscar)

logistic regression -> softmax classification (importante para pefiles(?))

classes (male or female e.g.) and dimensions (d = 3 e.g.) for word vectors to be classified

(Notation: dot means either full row or column)

cross entropy
