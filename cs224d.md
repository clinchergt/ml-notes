#2016 CS224D
## lecture 1

por que deep learning?

con machine learning normal, se pasa mucho tiempo describiendo la data, ajustando los parametros manualmente, especificamente para ese dataset o en mi caso tal vez incluso para ese especifico lenguaje (e.g ingles). deep learning permite que uno le diga al algoritmo esta es la data, y que el sepa describir y haga un prediccion por su cuenta, sin tener que describir manualmente. encuentra la respuesta si le doy el (raw) input y el output que deberia salir.

deep learning won't solve NLP tho (search andrew's post on reddit)

it also requires *a lot* of data

takes advantage of powerful computers (multicores, gpu help, etc.)

syntactical analysis: as opposed to traditional methods, every word is a vector, so every phrase a vector, so if two phrases are similar, hopefully the two vectors will be similar as well, which gives some meaning, some context to the actual phrase. in traditional methods we would only learn that something is e.g. a noun phrase.

semantical analysis: not having to understand subtle details. in traditional methods we might arrive at very convoluted way to represent (lambda calculus) and it's very discrete. there's no notion of similarity between phrases. in deep learning, you can actually make comparisons and predictions more directly, without having to define lots of subtleties. _everything's a vector_.


skewed distributions still affect deep learning, a good dataset is very important. basic probability, you need a proper representation of reality.

## lecture 2

wordnet has several problems also only english, subjective, subtleties are lost.

discrete representation lose also subtleties, similarities aren't well representative.

distributional similarity based representations: representing the words based on what surrounds them. old idea but very succesful in modern statistical nlp.

but how? collecting large text corpora. analyse n words to left, n words to right, "windows". common windows range from 5 to 10.

when picking how many dimentions, depends on the actual task. testing, see what happens as you go up or down. if the trend is up and i didn't continue, why? no reason not to. explore possibilities.

some very frequent words might skew the results. possible solutions include ignoring the words and capping the number they can get in the co occurrence matrix before applying svd


word2vec:

we try to predict words surrounding a particular word.

you optimize the log probability of any context word given the current center word

not very pretty to have two vectors per word, but it makes optimization easier, at the end you average both vectors out to get the final representation of the word.


## lecture 3

### word2vec; skip-gram model

simple word2vec alg consists of moving the window along the whole corpus (or corpora) and trying to predict the outside words. this means that each window has a center word, and a number of outer words. you try to predict the outer words based on the inner word.

after optimizing you will end up with 2 vectors for each word. one representing it as an outer word and one as a center word. in order to get the final representation you just average them or add them together. not too pretty but works well in practice.

updating the cost however involves calculating the gradient over every word in the corpus at every step. this is very expensive. if deep learning had neat simple convex functions the improvement would be fairly straight forward too.

in order to deal with the expensiveness of gradient descent we do stochastic gradient descent instead. we will almost always update the parameter after each window. small updates at each window.

btw, we initialize the word vectors as vectors of random small numbers (remember cs229)

since the updated matrix is so sparse, we just update the vectors that actually appear.

also, the normalization factor is _very_ expensive to compute as it sums over every word in the vocabulary. we instead we don't get all the words that don't appear in the current window, but just a few random words. it doesn't help much that e.g. 'zebra', doesn't co-occur with 'toaster'.

our cost function the becomes a maximization of the words in the window co-occurring in a log of the sigma function and then minimizing the cost of unrelated words co-occurring using the log of the sigma function of the negative inner product of those word vectors (taking advantage of the sigma(-x) == 1 - sigma(x) equivalence).

we use the sigma function because we're treating this as if it were a logistic regression problem. either words are related or not is basically what we're optimizing.

we also sample more frequent words more frequently in the negative samples as they should be more representative. you take their initial probability of occurring and raise that to the 3/4th power to decide how often to sample them.

#### continuous bag of words

another similar model. you try to predict the center word from the outside words instead of the other way around. it also works well.

you can even combine the skip-gram model and this one and average out the word vectors.

### glove

basically, fill out a huge co-occurrence matrix and use those numbers and subtract them from the word inner products and maximize that in order to get a similar result.

a caveat would be the very common pairs, so we have a maximum in the matrix. we don't want the very large counts to dominate the alg. in order to compensate for the very very common word pairs.

this is very scalable, because we only go over the corpus once. and then do all the math over the matrix, which is a lot smaller than the corpus.

### evaluate vectors

there are two ways, intrinsic and extrinsic evaluations.

the intrinsic tasks are just quick to compute, evaluate a specific task, there has to be some correlation between the metrics however. these help you understand the system.

the extrinsic tasks are actual useful tasks that would take long to compute and evaluate. these are basically real tasks. if you make one small change and the results improve overall, you can say that it was that small element that caused the improvement.

for instance, if you want to gauge how well you've picked the word vector dimensionality, you can vary it and test the result and see if there's any improvement in the measures, but intrinsic and extrinsic. since the former are faster to calculate, this is where they come in handy. again, make sure there's however some correlation between them and the actual real task you'll be doing.

## lecture 4

we have the normal training dataset with its input and output; the output belonging to a set of predefined labels we try to predict, like words, sentiment, etc.

we assume that the input x is fixed and optimize over the parameters W.

using matrix notation we get that f = W * x

why do we use cross entropy loss right now. squared error doesn't work well with NN for some reason. still not clear why. even if you want a specific number output as opposed to a label, it's better to turn it into a classification problem and average the various outputs to get a final number.

we obviously will also be using the regularized version of the loss function. it's very very very important to not overfit data.

what's also common in deep learning is learning the theta parameters *and* the word vectors. if however you have some initial word vectors but some of them don't appear in the testing data, then similar word vectors will diverge and we could end up misclassifying word vectors. you might also lose some of the intuition the vectors reflected initially, since now they will only reflect the end goal aspects of the word. (i.e. no grammatical similarities but sentiment similarities). for this reason if you have a small dataset, you should never update the word vectors, but if it's large enough, you can even skip the initial training of the word vectors (no Word2vec etc.)

### window classification
classify a word in its actual context of the window

you take the word and concatenate the surrounding words around it so form a longer vector. and you classify that(?) this results in a vector for the window that's R^5d, with 5 being the size of the window

you use any classifier you want and run it for the window vectors. but how do you update the word vectors?

always use the vectorize implementations. if you have more than one, test both and use timeit to figure out which one's best.

with softmax you only get a linear decision boundary which isn't great when you have lots of data, which is what we deal with in DL. so it'd be nice to have a non-linear decision boundary, this is why we will use neural networks, in fact they are so powerful that they will overfit very quickly, so get lots of data.

###NN

a neuron is basically a single small logistic regression unit. you have its input (which could include a bias term), a function and its output. a very simple neuron takes the input (which can be matrices) and outputs a value between 0 and 1.

so if we have different neurons, we can have different classifications running simultaneously. we don't even have to know exactly what we're trying to predict when using the intermediate neurons. the main idea however is to have another logistic regression function on top of those and have it decide what's actually important. it will help determine what the intermediate layers should capture.

non-linear boundaries are important because they allow us to model real life problems much more accurately, just adding linear models won't help much and will probably just keep resulting in more linear models.

NNs however can be dangerous in that overfitting is extremely easy, a few extra hidden units could overfit to the training data.

another important issue is that of regularization. the parameters for the NN need to be within a certain range, you can't them diverge so much, otherwise a single aspect of the model might get overestimated.

--------------
#2015 CS224D
##svd

- poder de computacion. matrices grandisimas. recursos?

- dificil agregar nuevas palabras/documentos. en un perfilador importante mantener al dia. lenguaje cambia, evoluciona.

- la geometria de las formas de palabras similares. no tienen misma estructura. no permiten hacer e.g. la misma operacion vectorial para llegar al pasado de un verbo.

- no es deep learning por asi decirlo

----

aprender directamente de la data los vectores reducidos.

back propagation ~= chain rule

recent simple faster: word2vec -> no counting, predict surrounding words.

----

##GloVe

intrincsic word vector eval

para responder calcular cosine distance de varios vectores. maximiar la distancia

no incluir el input (e.g. no incluir king en man:woman :: king:?)

depende del corpus, si habla mucho del que es correcto, lo lograra.

ambigüedad? dejar que el futuro modelo lo maneje *o* usar k means clustering (buscar)

logistic regression -> softmax classification (importante para pefiles(?))

classes (male or female e.g.) and dimensions (d = 3 e.g.) for word vectors to be classified

(Notation: dot means either full row or column)

cross entropy
