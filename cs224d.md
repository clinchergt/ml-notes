#2016

## lecture 1

por que deep learning?

con machine learning normal, se pasa mucho tiempo describiendo la data, ajustando los parametros manualmente, especificamente para ese dataset o en mi caso tal vez incluso para ese especifico lenguaje (e.g ingles). deep learning permite que uno le diga al algoritmo esta es la data, y que el sepa describir y haga un prediccion por su cuenta, sin tener que describir manualmente. encuentra la respuesta si le doy el (raw) input y el output que deberia salir.

deep learning won't solve NLP tho (search andrew's post on reddit)

it also requires *a lot* of data

takes advantage of powerful computers (multicores, gpu help, etc.)

syntactical analysis: as opposed to traditional methods, every word is a vector, so every phrase a vector, so if two phrases are similar, hopefully the two vectors will be similar as well, which gives some meaning, some context to the actual phrase. in traditional methods we would only learn that something is e.g. a noun phrase.

semantical analysis: not having to understand subtle details. in traditional methods we might arrive at very convoluted way to represent (lambda calculus) and it's very discrete. there's no notion of similarity between phrases. in deep learning, you can actually make comparisons and predictions more directly, without having to define lots of subtleties. _everything's a vector_.


skewed distributions still affect deep learning, a good dataset is very important. basic probability, you need a proper representation of reality.

## lecture 2

wordnet has several problems also only english, subjective, subtleties are lost.

discrete representation lose also subtleties, similarities aren't well representative.

distributional similarity based representations: representing the words based on what surrounds them. old idea but very succesful in modern statistical nlp.

but how? collecting large text corpora. analyse n words to left, n words to right, "windows". common windows range from 5 to 10.

when picking how many dimentions, depends on the actual task. testing, see what happens as you go up or down. if the trend is up and i didn't continue, why? no reason not to. explore possibilities.

some very frequent words might skew the results. possible solutions include ignoring the words and capping the number they can get in the co occurrence matrix before applying svd


word2vec:

we try to predict words surrounding a particular word.

you optimize the log probability of any context word given the current center word

not very pretty to have two vectors per word, but it makes optimization easier, at the end you average both vectors out to get the final representation of the word.


## lecture 3


--------------
#2015
##svd

- poder de computacion. matrices grandisimas. recursos?

- dificil agregar nuevas palabras/documentos. en un perfilador importante mantener al dia. lenguaje cambia, evoluciona.

- la geometria de las formas de palabras similares. no tienen misma estructura. no permiten hacer e.g. la misma operacion vectorial para llegar al pasado de un verbo.

- no es deep learning por asi decirlo

----

aprender directamente de la data los vectores reducidos.

back propagation ~= chain rule

recent simple faster: word2vec -> no counting, predict surrounding words.

----

##GloVe

intrincsic word vector eval

para responder calcular cosine distance de varios vectores. maximiar la distancia

no incluir el input (e.g. no incluir king en man:woman :: king:?)

depende del corpus, si habla mucho del que es correcto, lo lograra.

ambigÃ¼edad? dejar que el futuro modelo lo maneje *o* usar k means clustering (buscar)

logistic regression -> softmax classification (importante para pefiles(?))

classes (male or female e.g.) and dimensions (d = 3 e.g.) for word vectors to be classified

(Notation: dot means either full row or column)

cross entropy
