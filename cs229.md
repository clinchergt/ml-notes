#cs229
##intro
mate 3 y algebra lineal

the normal equations method doesn't escale as well as gradient descent

feature scaling very important if you're using polynomial models (x + x^2 + x^3 etc.)

learning rate, you debug it by plotting the cost function as you iterate, every iteration must be going down
if not, then pick a smaller alpha value (learning rate)

sometimes you get to mess with the features, e.g. if you have house dimensions, instead of using them individually you can use the area isntead.

normal equations sometimes can give you a better method of finding a minimum. if the number of features isn't too large, it's the way to go (anything larger than 10k, you should probably consider gradient descent)

pinv(x'*x)*x'*y

if x'*x isn't invertible, you could have redundancy (linearly dependent) features. you could also have too many features, more than you have examples.

##classification:

we have a new hypothesis: h(x) = g(z) where g = 1 / (1 + e^-z)

where z is the previous hypothesis: theta' * x

we also have a decision boundary, which is defined since if g(z) >= 0.5 we predit y=1 and if not then y=0

this in turn means that any time z >= 0 that resulst in the prediction being y=1

this also allows us to plot the decision boundary.

due to the nature of our new hypothesis, the old cost function turns out not to be convex so there's no
guarantee that gradient descent will arrive at a global minimum, but instead will get stuck at local minimums

we use instead the cost function:

cost(h(x), y): -log(h(x)) if y == 1; -log(1-h(x)) if y == 0

cost function is now convex, which is what we wanted. (stats theory: principle of maximum likelihood estimation)

the gradient descent updating equation turns out to be the same, except that obv the definition of the
hypothesis function is different

###multiple class classification problems

for multiple class classification problems, you run the process for each class in a one vs all manner, so that you end up with a multiple hypothesis, each optimized by whichever algorithm. in order to make predictions you run each one and pick whichever is highest, which shoudl result in giving you the right prediction for that example, i.e. predict the right class.

## under and overfitting

underfitting means that your model doesn't suit the training or real data very well, you need to modify it so it better reflects the training data

overfitting however is modifying the model so that it fits the training data too well, so well it might disregard some patterns that could make it fail to make accurate predictions to examples outside the training data. so the cost function could be really close to 0, but the model would still fail to make accurate real life predictions.

### possible solutions

reducing the number of features, either by manually selecting which are more important (not really recommended) or by using a model selection algorithm (will elaborate later)

you can also apply regularization, i.e. keep all features but reduce their magnitude, assign them weight, works well if every feature is important, even if it's just in a minimal manner.

##normalization

you penalize features in the cost function so that they don't overly affect the cost. you don't, however, penalize theta_0, per convention. you penalize by adding the sum of the square thetas, and multiply that by a regularization pattern, lambda. this basically results in smooothing out the curves of the model.

a large enough lambda however can result in underfitting.

##neural networks

we use neural networks because real life examples have lots of features, which would result in a massive need for computational power and doesn't scale well

a neuron is basically a processing unit that receives some input(s) and outputs something to other neurons

a neural network basically allows you to have it learn its own features and then use those in the linear regression-like model

forwards propragation

the way networks are connected is called architecture.

there are input layers, output layers, and hidden layers.

### cost function and partial derivatives

l - layer
L - number of layers
m - number of examples

the cost function is gonna sum over both the training examples and each layer of the neural network. the regularization term will also sum over each node of every layer (skipping the bias node)

in order to calculate the partial derivatives we use the backpropagation algorithm. for this we will calculate the terms delta_j superscript l, which represents the error of node j in layer l. e.g. delta^(L) = a^(L) - y^(L). superscript, not powers. and for the others deltas:

delta(i) = (Theta(i))'*delta(i+1) .* g'(z(i))  --- delta(1) doesn't exist

### gradient check

we use a numerical way to check the gradients, and if they're approximately the same, we can be sure that the gradient implementation is good. we do this by doing the following:

gradplus(i) = grad(i) + epsilon
gradminus(i) = grad(i) - epsilon
aproxGrad = (gradplus - gradminus) / 2*epsilon

if we do this for every gradient and they're all roughly equivalent, we're set.

IMPORTANT: turn off the gradient check once you start the actual training as it is very slow to check.


### random initialization
we initialize with random weights because initializing with zeros results in backpropagation getting stuck, as the partial derivatives are the same, so after each iteration the weights would change but would remain the same relative to each other.

### final notes on NN

J(Theta) is a non-convex function so it's possible that the alg gets stuck in a local minimum, but in practice this doesn't seem to happen often.

## Machine learning diagnostic

evaluating and improving model and which avenue to take in order to improve performance and results.

### evaluating a hypothesis

splitting the dataset into two parts: training and testing sets. a 70/30 split is good.

first, learn the parameters from the training data, then compute test set error

(for logistic regression you could also use the missclassification error)

err(h(x), y) = 1 if h(x) >= 0.5, y=0 or h(x) < 0.5, y =1; 0 otherwise. you then count the misslabeled examples


mobile selection problems: which features to use, what degree polynomial, etc.

picking d (dimension) based off the training data. getting theta for each d from the *training set* (obv) and then get the cost from the *test* set and see which one works best. but then this would result in optmiizing for the test set.

we instead then split the dataset into three pieces. training set, cross validation set, and test set. 60/20/20 splits are fine.

we now get thetas from the training set, then see how well each hypothesis does on the cv set, pick the best one out of those, and finally use the test set to get the generalized error rate.

### diagnosing bias/variance

i.e. under and overfitting respectively

plotting the error from the training data and the cv data gives some insight into this

regularization (already previously discussed) but we need to know how to pick a good lambda value.

calculate theta for various lambda values, take each hypothesis and use the cv set to evaluate them

### learning curves

you artificially limit the number of examples yo have to train the model, and plot the error on the training data as you increase the number of examples and also plot the cv error using each model

reveals stuff

if cv error is much higher than train error, more data could help

### options

get more training examples - fixes high variance
try smaller sets of features - fixes high variance
try additional features - fixes high bias
try adding polynomial features - fixes high bias
try decreasing lambda - fixes high bias
try increasing lambda - fixes high variance


### NN

http://i.imgur.com/XXf5MUX.png


## spam filter

start with something quick and dirty, spend no more than a day on it. plot the learning curves and see what could benefit the model the most.

also error analysis: manually examine examples that your model was wrong on and see if you can spot any systematic trend. e.g. see what type it is, what features could've helped the model do better on them

## Skewed classes

when one case is way better represented in the dataset, so much that something that isn't even machine learning could be better at predicting simply by predicting always one case, you get skewed classes.

if you have skewed classes it's hard to tell if your alg is doing something right or if it's just doing something extremely simple like predicting y = 0 all the time, you need a better error indicator

### precision/recall

precision: of all the examples taht were predicted y = 1 actually have y = 1; true positives / (predicted positive)

recall: of all the examples that y = 1, what fraction did we predict correctly that y = 1; true positives / (actual positives)

### trading off presicion and recall

we want to predict y = 1 only if very confident. we could do this by upping the threshold and say y = 1 only if h(x) >= 0.7 as opposed to 0.5. this will lead to higher precision but lower recall.


supposed we want to avoid missing cases where y = 1 (avoid false negatives). in this case we want to instead lower the threshold e.g. y = 1 if h(x) >= 0.3. this will lead to lower precision but higher recall.



how to compare precision/recall numbers? F1 score (F score): 2 * PR / (P + R)

so you try different values of the threshold and get the F score on the cv dataset and pick the one that does best.

## Data for machine learning

when is a lot of data useful?

can a human expert confidently predict y if given x and only x. does x suffice to predict accurately? if not, the more data might not help significantly.

unlikely to overfit if you have a very large training set (Jtrain(theta) ~= J_test(theta). using alg with many parameters (like a neural network with lots of hidden units) (Jtrain will be small)

with these two combined you can get a low J_test(theta)


## support vector machine


you don't use lambda anymore to regularize the parameters (the second term) but you use another constant (C) to regularize the first term, the sum of errors in the cost function.

the hypothesis in an svm will output either 0 or 1, will output 1 if theta' * X > 0, otherwise it will output 0. so this means that if z >= 1 cost1 will be 0, and if z <= -1 cost0 will be 0 http://i.imgur.com/DLOxu1M.png

if C is a large number, we want that sum term is close or exactly 0 in order to minimize.

svm's will then predict a more solid decision boundary, with a larger margin. you could say it generalizes better.


###Kernels

given x computing new features depending on the proximity landmarks. l(1), l(2), l(3) (lana one two and three) 

f1 = similarity (x, l(1)) = exp (- eucledean distance squared over 2 sigma squared) = k(x, l(i)), all of that ignoring x0


if x ~= l: f1 is gonna be close to exp(0) and so close to 1

if x is far from l(1): f1 = exp(large number / 2sigma) ~= 0

where do we get landmarks from?  we take the training examples and put landmarks at their locations, so you end up with m landmarks.


####using kernels

- you need to choose C
- you need to choose the kernel 

no kernel would mean a linear kernel i.e. theta' * X

a gaussian kernel is the one we talked about previously -- need to choose sigma

- write the kernel function (gaussian kernel function might be included in library)

NOTE: feature scaling is important (especially when using the gaussian kernel)

NOTE2: whatever kernel you may choose, the condition called mercer's theorem needs to be satisfied

for multi class classification you can use the built-in methods or just use the one vs all method.

if n is small and m is intermediate, svm's might be a good idea
if n is small and m is large, using logistic regression or svm without a kernel might be a good idea

neural networks would work well with all these settings but the training might be slower.


## unsupervised learning

data with no labels, that is, we don't have a y vector, we just have X

### k means clustering

you pick randomly cluster centroids and assign points closest to them to it. once that's done you move them
to the average point for the examples that belong to it and reassign the examples closest to it. repeat this until the alg converges (i.e. centroids don't move anymore and the assignment doesn't change)

input is as follows:

K (number of clusters)
training set (X)

x(i) belongs to R^n (we drop x0)

u1 (mu1), u2, ..., uk are the cluster centroids

if at some point you end up with a cluster centroid with no points assigned to it you can either eliminate it
or reinitialize it

k means can also work on non-well-separated clusters, like when doing market segmentation.

### optimization objective

the cost function is a function of c(i) and u(k) = 1/m sum from 1 to m of the norm of x(i) - u(k) squared.

this is the assignment phase. the moving centroids phase can also be solved by minimizing J(...)

### random initialization

randomly pick K examples, set u1, ..., uK to those locations.

however, depending on the initialization, the alg is prone to local optima if you're unlucky. to deal with this you can initialize more than once and run the alg more than once (common to do so 50 to 1000 times) once you're done, you pick the one that gives the lowest cost.


### choosing K

the elbow method: you run the alg multiple times with different numbers of clusters. a plot of the costfunction as you increase the number of clusters. you will see that the cost goes down. at some point you will see an 'elbow' in the graph, were the improvement will slow down considerately. however in practice you don't always see an 'elbow', there isn't always a clear answer.

maybe you're running k-means for some later purpose (e.g. market segmentation). often a better way to pick K is to see how different numbers of clusters affect that downstream purpose.

## Dimensionality Reduction

getting rid of redundant features. this is done by project e.g. 2D features to 1D so you could have:

x(1) € R^2 --> z(1) € R
x(2) € R^2 --> z(2) € R
x(m) € R^2 --> z(m) € R

this saves resources and thus allowing us to run the algs more quickly.

more usually the reduction goes from e.g. 1000D to 100D.

NOTE: PCA is not linear regression, despite the similarities. pca tries to minimize the distance to the line perpeducularly, not vertically. hence the algs are different and do not provide the same results.

### alg

- normalize
- feature scaling
- compute the covariance matrix
- compute the eigenvectors with svd()

svd gets you the u vectors. you use the first K vectors.

z = Ureduce' * X;

to reconstruct from the data you could do

Xapprox = Ureduce * z;

### notes

picking k: you want to pick the k that allows the ratio between squared projection error and variation in the data to stay <= 0.01

so you start with k=1 and calculate what is kept, if you get 99% you pick k=1 if not you do k++ and try again. this procedure can be inefficient.

but if you sum the Sii values from 1 to k from the svd alg (S) and divide them by the values from 1 to n you get the accuracy kept.

some applications would be to compress data to reduce resource needs. you can speed up the alg. and finally it helps with visualization.

a *bad* usage of pca is trying to use it to prevent overfitting.


## application: anomaly detection

we get a set of normal examples and process them.

we then get xtest, and we have to determine whether it is an anomaly, an outlier, or if it's normal.

so we model p(x); 

if p(xtest) < epsilon -> we flag it as an anomaly
if p(xtest) >= epsilon -> okay


due to skewed datasets, we use f1 score to pick the best epsilon. simple accuracy isn't a good enough measure

### anomaly detection vs supervised learning

if you have a very small number of positive examples (y=1), also if you have a large number of y=0 you use the anomaly detection.

also when there's many different types of anomalies. so the anomalies in the trianing set might not look anything like other anomalies outside the training set (e.g. the ones in the test set)

on the other hand, if you have a large number of positive and negative examples you should go with supervised learning.

if you have enough positive exmaples and future positive examples are likely o be similar to the ones in the training set.

### choosing features

if data isn't very gaussian looking you can modify it and e.g. take the root square and get it to look more gaussian or use the log.


### multivariate gaussian distribution

when analyzing variables separately, even tho together they might create an outlier, might not look extraordinary if you analyze individually.

so you don't model p(x) separately but in one go. so u € R^n and sigma € R^n*n (covariance matrix, which determines how 'fat' and how tall the distribution is). the u (means) define the center of the distribution


if you compare the two models, you can conclude that the older model is cheaper computationally. it is generally more used. and you can create new features in order to capture correlations without using the multivariate model.

the normal model scales better to large numbers of features.

for the multivariate if n > m the matrix is not invertible. this is also true if you have redundant features.

also, in practice m shoudl be >> than n. so that it's worth it.


## content based recommendations

we use content to predict e.g. the rating a user will give to a movie and hence recommend the ones that would core higher.

we do this by using the variables r(i,j) (whether user j rated movie i), y(i,j) (the actual rating), x(i) is as always the movie feature vector, theta(i) is the parameter vectors learned by our alg.

this is basically a linear regression problem. except u learn parameters for every user, so we add an extra sumation over all the users and minimize the cost function.

### collaborate filtering

this is called collaborate filtering because you will be helping the system be more accurate as you participate more and rate.

you can either learn theta from a set of features x or given theta learn a set of parameters theta. which comes first?

you can make an initial guess as to what theta could be, estimate the features, reestimate theta, and so on.

to be more efficient you can minimize both simultaneously and avoid the iterations described above.

NOTE: since we are learning all the features, we get rid of x0, since there's no need for it. if it's necessary, the alg will learn it on its own.

in order to get started, we initialize in features and parameters to small random values. this allows for features to be linearly independent.


h(x) = X * Theta'
low rank matrix factorization (vectorized alg)

NOTE: since you are learning the features, you can also find related products. if you get the norm of the difference between two feature vectors and you find that the distance is small, you can conclude that they are related.

### Mean normalization

you use the average for each score to end up with a Y that uses the actual rating minus the average as its rating. you then learn theta and x for each movie and user. in order to predict you now use however

theta' * x + muu;

this will solve the problem of predicting 0 for every movie to a new user.

## large datasets

### stochastic gradient descent

since computaionally expensive, we do this instead of doing the classic way

cost(theta, x, y) = 1/2 (h(x) - y)^2
Jtrain = 1/m sum of cost(theta, xi, yi)

1. randomly shuffle dataset
2. thetaj = thetaj - alpha (h(x) - y)*xj
3. goto 2

so you only look at one example per iteration and not every example and sum them.

it will not converge per se, but will dwell around the global minimum

### mini batch gradient descent

instead of using just one example per iteration, you use a small number of exmaples, e.g. 10

during learning, right before updating theta, you can calculate the cost, and every 1000 (e.g.) iterations you can plot the cost of average of the last examples.

NOTE: as opposed to regular gradient descent, you can slowly decrease alpha as you go thru the iterations in stochastic descent. however you might have to waste time fiddling with the decreasing parameter tho.


### Online learning

learn the probability of a client doing something given a set of conditions p(y=1|x; theta) so we can optimize the parameters like the price of the product.

you can use an example and then never look at it again.

### Map reduce and data parallelism

you divide the training set in the number of computers you're gonna use (e.g. 4)

each computer calculates their fraction of the corresponding gradient sum and/or cost function sum, the result then gets sent back to a central pc and it finishes the calculations.

many algs can be expressed as sums, so we can distribute the task easily. you can also distribute the sums among cores in a single computer.

NOTE: don't forget that vectorization always helps.


## Photo OCR

stands for optical character recognition. the problem is divided in several parts

1. text detection
2. character segmentation
3. character classfication

the division of this problem forms what we call the pipeline

### Sliding windows

the problem of text detection is tough because of the aspect ratio of the text can vary by a lot, as opposed to e.g. pedestrian detection in photos.

for pedestrian detection you use a set window and shift it along the image using a step/stride size. once you're done you use larger image patches (larger windows, scaling it up).

you can use sliding window in order to do character segmentation in order to detect separations between chars

### Getting lots of data and artificial data

in the OCR problem and similar problems, you can end up with datasets that might not be large enough, but that can give you an idea of what real data looks like, so you can effectively fabricate your own data using font libraries, random backgrounds and blurring functions and you end up with a synthetic dataset.

you can also take prexisiting examples and add artificial distortions (within reason) to the image and get new examples out of that.

the distortions you introduce should be distortions that you might run into in the testing.

- focus on getting your model to have low bias. use more features/hidden units before working a lot on getting more data.
- there will be a way to get more data, it's usually not *too* hard (fabrication, collecting and labelling yourself, crowd source)


### Ceiling analysis

get a measure of the overall system, manually make a phase perfect and see how much the performance increases

you then can pick which area would benefit you most to work on


--------------
